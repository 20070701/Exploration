{"cells":[{"metadata":{"_uuid":"056c8d8ef3c91f901ed0bca2db3d960920df34dd"},"cell_type":"markdown","source":"When dealing with multiple features, simple linear regression loses its charm and so Multiple regression is necessary for encapsulating the effect of multiple features.\n\n"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","collapsed":true,"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"markdown","source":"Now, getting the data.\n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"67cd3f23d021c3505d27f4a0c70c19118e9ac276"},"cell_type":"code","source":"from sklearn.datasets import load_boston\nboston_data = load_boston()\ndf =pd.DataFrame(boston_data.data,columns=boston_data.feature_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3a1b64e5b75362ec9fba5a5bfdb45b5b55ee4cef"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66563b9229927792db68103e6227c548439db618"},"cell_type":"markdown","source":"Then, for simplicity, I’ll be using X and y to denote the feature and target variables."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"40b971257be6ef09387305f8d3f9d6e6ea484fee"},"cell_type":"code","source":"X = df\ny = boston_data.target","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"220aa974eb8844712f90c302d34c7eace2f58963"},"cell_type":"markdown","source":"Now, let’s add some extra constant term to allow statsmodels to calculate the bias."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"a81d5560864e03300c5fc3521e5168f23e0353cb"},"cell_type":"code","source":"X_constant = sm.add_constant(X)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2c916f905055114b4239eb6c2039a481dd81576f"},"cell_type":"markdown","source":"Now, let’s instantiate and fit our model with an ordinary least square model.\n\n"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ffb6b5b314af253b5b17034c9e43dabd076ffe30"},"cell_type":"code","source":"model = sm.OLS(y, X_constant)\nlin_reg = model.fit()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66c79b4c303196c16c0101b953de2e12742cb3c9"},"cell_type":"markdown","source":"To see the results, run the following code:\n\n"},{"metadata":{"trusted":true,"_uuid":"2b965aa4de26637f0fdab711fc93f443aed3e72e"},"cell_type":"code","source":"lin_reg.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"307f7ffd8571b5bfa404cd9d04b61e19445694ce"},"cell_type":"markdown","source":"Another way to make your model would be to actually use the formula,\n\n"},{"metadata":{"trusted":true,"_uuid":"e667fd74af652e0c6b7f09d17143010022d7606e"},"cell_type":"code","source":"f_model = smf.ols(formula = 'y ~ CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT', \n              data=df)\nf_lin_reg = f_model.fit()\nf_lin_reg.summary()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"233f046777114b39e0a926bb9a37741fd37a4d5f"},"cell_type":"markdown","source":"To see what the actual prediction for 10 rows of the features dataframe looks like, run these:"},{"metadata":{"trusted":true,"_uuid":"4d967fb3ab75555fcaaa259abea760531762ffca"},"cell_type":"code","source":"print(lin_reg.predict(X_constant[:10]))\nprint(f_lin_reg.predict(X_constant[:10]))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9413b0f52e93d42def48ad73d000fe3a1b75e1f7"},"cell_type":"markdown","source":"There you go! That’s how you perform Multiple Regression using Python!"},{"metadata":{"_uuid":"84a163eaa6a33fa2e9cc50b192f2588c06276783"},"cell_type":"markdown","source":"(Just an add-on. If you are interested about which feature is important then go through this.)"},{"metadata":{"_uuid":"ad5834f5828737b2cc1b90faa8efba2bdf68a60a"},"cell_type":"markdown","source":"Let’s look at the correlation between the different features.\n\n"},{"metadata":{"trusted":true,"_uuid":"1c4f99587fe4d4a9706e6813c28b0cce435e5c28"},"cell_type":"code","source":"pd.options.display.float_format = '{:,.4f}'.format\ncorr = df.corr()\ncorr[np.abs(corr) < 0.65] = 0\nplt.figure(figsize=(16,10))\nsns.heatmap(corr, annot=True, cmap='YlGnBu')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fb5cb04e6a0b384555a65073c0d277b959234a0"},"cell_type":"markdown","source":"Let’s also try using the R2 score to see the importance of features.\n\n"},{"metadata":{"trusted":true,"_uuid":"723abf9f08ac547231cea1e7b62ba95ece0ad51b"},"cell_type":"code","source":"from sklearn.metrics import r2_score\nlinear_reg = smf.ols(formula = 'y ~ CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B + LSTAT', \n              data=df)\nbase = linear_reg.fit()\nprint(r2_score(y, base.predict(df)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"62cc8b4d401d530cda40292ecf2867917e28b089"},"cell_type":"markdown","source":"The R2 score that I got was 0.7406 and now this will serve as my base number. If the removal of a feature changes the R2 score significantly then that feature can be termed as an important feature.\n\nFor e.g. if from the above formula, LSTAT is removed then the R2 score drops to 0.68 whereas if AGE is removed it stays at 0.74. Hence, LSTAT is an important feature."},{"metadata":{"trusted":true,"_uuid":"364c69ec8618e389c1cd27f08d6a47c040b5939b"},"cell_type":"code","source":"# WITHOUT LSTAT\nlinear_reg = smf.ols(formula = 'y ~ CRIM + ZN + INDUS + CHAS + NOX + RM + AGE + DIS + RAD + TAX + PTRATIO + B', \n              data=df)\nbase = linear_reg.fit()\nprint(r2_score(y, base.predict(df)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"608fba714d6c362cb5d9fb59b07e79639564f15f"},"cell_type":"code","source":"# WITHOUT AGE\nlinear_reg = smf.ols(formula = 'y ~ CRIM + ZN + INDUS + CHAS + NOX + RM +DIS + RAD + TAX + PTRATIO + B + LSTAT', \n              data=df)\nbase = linear_reg.fit()\nprint(r2_score(y, base.predict(df)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ed15b31c9d1d2fcc1618f870a6f91a6837d442f"},"cell_type":"markdown","source":"These are some ways you can check for feature importance and see if feature engineering can help make better features that uplifts the model."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"b5df6b362b5520262bf28248ea2dfca88570bc80"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}